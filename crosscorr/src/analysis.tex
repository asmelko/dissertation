\section{Problem Analysis}\label{sec:analysis}

The design and implementation of an optimal solution are affected by several aspects of the problem. Furthermore, different scenarios of computing multiple signals being cross-correlated simultaneously benefit from different approaches. In this section, we provide an overview of the most important optimizations which are essential in our proposed algorithms.

For the sake of simplicity, we will focus solely on 2D cross-correlation since 1D correlation is significantly less interesting and all the proposed optimizations can be extended into higher dimensions easily. The input signals are discrete, so we will refer to the materialized inputs as \emph{matrices} to take advantage of the most familiar terminology available.


% -----------------------------------------------------------------------------
\subsection{Workload parallelization}\label{sec:analysis-workload}
% -----------------------------------------------------------------------------

A single cross-correlation (one-to-one) produces one output matrix, where each element corresponds to one possible relative shift between the input matrices. An example with two $4\times 4$ matrices is depicted in Figure~\ref{fig:cross_corr_shifts}. The value of the output element is computed as a sum of an element-wise multiplication performed on the overlapping area of the two input matrices.

\begin{figure}[ht]
	\fontsize{6}{8}\selectfont
	\centering
	\def\svgwidth{0.6\textwidth}
	\input{crosscorr/src/img/overlap-Shifts.pdf_tex}
	\caption{The output matrix with corresponding relative shifts of input matrices}
	\label{fig:cross_corr_shifts}
\end{figure}

The individual operations can be decomposed in a tree as indicated in Figure~\ref{fig:cross_corr_one_to_one_tasks}. The top-level node (green) represents the computation of a single output matrix. The second level (orange) represents computations of individual elements in the output matrix (different input overlaps). The third level (yellow) corresponds to the elementary multiplications performed on the overlapping area.

\begin{figure}[ht]
	\fontsize{5}{6}\selectfont
	\centering
	\def\svgwidth{0.65\textwidth}
	\input{crosscorr/src/img/overlap-Tasks.pdf_tex}
	\caption{A work decomposition of one-to-one cross-correlation}
	\label{fig:cross_corr_one_to_one_tasks}
\end{figure}

Both the first and the second levels comprise independent operations --- i.e., operations that can be performed without explicit data synchronization. The operations on the third level (multiplications) need to be reduced into the result at the second level (using a sum as the reduction operation). In case of more elaborate scenarios (\textit{one-to-many}, \textit{n-to-m}, and \textit{n-to-mn}), the tree in Figure~\ref{fig:cross_corr_one_to_one_tasks} is merely extended into a forest of independent trees, thus enabling another level of parallelism. Although this decomposition may indicate the problem is embarrassingly parallelizable, there are two major concerns for any GPU-accelerated implementation:

\begin{itemize}
	\item The workload at the second level is highly irregular. Corner elements are computed by a single multiplication whilst the elements in the center of the output matrix require a full element-wise multiplication and sum-reduction of the input matrices. This imbalance may cause serious code divergence (i.e., suboptimal performance) in the warps and thread blocks.
	\item The problem is highly data-bound as each loaded element is used in a simple elementary operation (multiplication and addition). This might create a significant underutilization of the GPU cores if each core spends most of the time waiting for the data.
\end{itemize}

Therefore, our objective is to design algorithms that will heavily reuse loaded input data whilst attempting to provide a better workload balance, especially at the warp level.


\subsubsection{Workload distribution}

One of the key aspects that affect the efficiency of GPU-based parallel programs is workload decomposition and distribution among the allocated threads. It defines the level of parallelism (since the threads are executed concurrently), synchronization (when one thread needs to wait for the results of another thread), and native parallel cooperation (via shared memory or warp-level instructions). It also affects registry allocation and, transitively, data reuse (since the input data needs to be moved from global memory to registers).

To simplify the description of the subsequent optimizations and algorithms (especially their approach to data reuse), we adopt the \emph{task} abstraction for the workload division and the work assignment to computing elements, where \emph{task} is usually an element of work performed by one CUDA thread.

\emph{Task} comprises a~well-defined group of nodes from the work decomposition schema introduced in Figure~\ref{fig:cross_corr_one_to_one_tasks}. The most straightforward implementation would map one \emph{overlap} (orange element at the second level) to one task and we denote this the \textbf{overlap-wise} (or simply the \textbf{overlap}) strategy. More elaborate task definitions, that would allow better input data reuse, will be outlined later using the overlap-wise strategy as the reference point.

In special cases, it is possible to assign each task to a group of threads (a~warp or a~block). This may provide a simpler approach to algorithms where more fine-grained division of tasks is impractical, but when each task may be processed in a cooperative or even SIMD-like manner. In the follow-up descriptions, we always assume that a task is assigned to one thread; unless we explicitly state otherwise.


% -----------------------------------------------------------------------------
\subsection{One-to-one data reuse}\label{sec:analysis-reuse}
% -----------------------------------------------------------------------------

The problem of the data-bound nature of the cross-correlation definition-based algorithm can be mitigated by smart caching of the input values which we generally refer to as \emph{data reuse}. In other words, every input value loaded into the registers or shared memory should be reused in multiple computations (multiplications) to improve the ratio between load and arithmetic instructions. In the following, we will introduce several task-forming strategies (i.e., how the cross-correlation individual operations are assigned to CUDA threads) that are designed to enable data reuse.

The most straightforward strategy (\textbf{overlap-wise}) assigns one relative input overlap (one sum of overlapping products) to a CUDA thread. Analyzing the input data access patterns, we have made an observation, that adjacent overlaps share significant portions of input data (as illustrated in Figure~\ref{fig:overlap_grouped_shared_data}).

\begin{figure}[ht]
	\centering
	\def\svgwidth{0.7\textwidth}
	\fontsize{7}{9}\selectfont
	\input{crosscorr/src/img/overlap-DataSharedByOverlaps.pdf_tex}
	\caption{Input data shared between neighboring overlaps}
	\label{fig:overlap_grouped_shared_data}
\end{figure}

The overlap-wise strategy can be implemented with data-reuse optimizations if the threads processing adjacent overlaps can share or exchange the input data efficiently (using shared memory or warp-shuffles). Another possibility is to create larger tasks that would aggregate multiple adjacent overlaps in a task so that a thread does not have to share the inputs with neighbors for data reuse as the reuse happens on loaded data internally. We have denoted this strategy \textbf{grouped-overlap}. For the sake of simplicity, we have selected only one dimension (which is depicted in Figure~\ref{fig:overlap_grouped_shared_data}) where the task aggregates the overlaps with the same relative column displacement and adjacent row displacements. We have considered more complex aggregations, but using a row-wise approach to grouping is much simpler to implement as we demonstrate later in Section~\ref{sec:grouped-overlap-alg}.
% TODO - warp shuffle v principu pouziva stejny princip ale col-wise. Nejak dopsat do textu.

% -----------------------------------------------------------------------------
\subsection{Fine grained parallelism}
% -----------------------------------------------------------------------------

In most cases (especially when multiple cross-correlations are computed simultaneously), the overlap-wise strategy gives us a sufficient amount of tasks to easily saturate the GPU. In fact, most of the data reuse strategies exploit some form of grouping --- i.e., one task groups computations of multiple overlaps together. In the case of smaller instances (especially in one-to-one cross-correlation), the total number of tasks may decrease so that the GPU is no longer entirely saturated. In such cases, we can choose a more fine-grained workload division. Each overlap computation is basically an element-wise sum of a Hadamard product of the overlapping part of the input matrices. The sum itself is associative, so we can compute the Hadamard product by multiple threads (concurrently) and then use a parallel sum reduction to get the result.

There are many ways to divide the Hadamard product, perhaps the most straightforward is to divide the matrix of products into stripes of adjacent rows (of a constant height, except for the last stripe). This strategy is denoted \textbf{split-row} and the height of the row stripes can be selected as a tuning parameter of the algorithm. Analogically, we define \textbf{split-col} strategy, which uses the same approach but creates stripes of columns instead of rows, and we refer to the principle of both strategies as \textbf{split-overlap}.

Technically, the split-overlap principle can be combined with the previously mentioned data reuse grouped-overlap; however, this often turns out to be counterproductive. The split-overlap is used in case the GPU is not saturated and in such cases, creating enough work for the threads is far more important than data reuse (i.e., the subsequent grouping of the overlap stripes does not improve the performance).

An alternative approach to split-overlap is to use basic overlap-wise task definition but assign a whole warp of threads per task. The warp divides the individual task elements (i.e., the products) among the threads evenly, while each thread accumulates its partial sum in its register. Finally, the threads employ warp instructions for the final reduction. This approach is used in a specialized \textbf{warp-per-overlap} algorithm (Section~\ref{sec:warp-per-overlap}).


% -----------------------------------------------------------------------------
\subsection{Processing multiple inputs simultaneously}\label{sec:analysis-multimat}
% -----------------------------------------------------------------------------

When multiple cross-correlations are being computed simultaneously (i.e., in \textit{one-to-many}, \textit{n-to-m}, and \textit{n-to-mn} scenarios), another level of data reuse becomes available. We can create tasks that aggregate computations using the same overlaps (relative dislocations) but on different input matrices. The main advantage is that the sizes of the overlapping inputs are exactly the same so this strategy does not negatively affect load balancing. Based on the application scenario, we have decided to explore two possible strategies:

\begin{figure}[ht]
	\centering	
	\begin{subfigure}{0.42\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\fontsize{7}{9}\selectfont
		\input{crosscorr/src/img/overlap-JobSizeOverlapSimple.pdf_tex}
		\caption{Grouping in \textit{one-to-many} configuration}
		\label{fig:job_size_modifiers_n_to_mn}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.42\textwidth}
		\centering
		\def\svgwidth{\textwidth}
		\fontsize{7}{9}\selectfont
		\input{crosscorr/src/img/overlap-JobSizeBoth.pdf_tex}
		\caption{Grouping in \textit{n-to-m} configuration}
		\label{fig:job_size_modifiers_n_to_m}
	\end{subfigure}
	
	\caption{Multi-matrix approach and its combination with overlap grouping}
	\label{fig:job_size_modifiers}
\end{figure}

\begin{itemize}
	\item \textbf{multi-matrix-right} uses multiple right matrices for each overlap (i.e., each value loaded from the left matrix is used in multiple correlations with right matrices). This approach will work in all three multi-matrix scenarios (\textit{one-to-many}, \textit{n-to-m}, and \textit{n-to-mn}).
	\item \textbf{multi-matrix-both} uses multiple left and right matrices so that each value from the left is used with all right matrices and vice versa. This approach is designed solely for the \textit{n-to-m} scenario.
\end{itemize}

Figure~\ref{fig:job_size_modifiers} shows the multi-matrix strategies and compares them with the grouped-overlap. Furthermore, the multi-matrix strategies can be combined with the grouped-overlap strategy as well as the split-overlap strategy.
