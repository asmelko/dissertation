\chapter{Employing GPUs in scientific algorithms}

The information age has brought a massive increase in the amount of data that is being collected and processed. The `data explosion' can be observed in virtually every field of computer science. In the scientific domain, this phenomenon is multiplied by increasingly powerful data-gathering devices (i.e., cytometers in bioinformatics, optic sensors in physics, seismometers in geology, etc.). The amount of data is simply too large to be processed in the required time. In order to alleviate this apparent pressure, the corresponding algorithm (which typically has super-linear time complexity) must be ported to massively parallel architectures, such as GPUs. The provided advantage can result in the ability to assess bigger data, use more detailed processing methods, or analyze the data in real time.

This chapter enumerates four selected scientific algorithms and discusses their challenges in porting to GPUs to enable higher performance.

\section{Hierarchical clustering with the Ma\-ha\-la\-no\-bis linkage}

In the field of bioinformatics, hierarchical clustering is a popular method for analyzing various types of data. 
In general, hierarchical clustering is an unsupervised machine learning method that aims to group the data points into clusters according to some \emph{linkage criterion}.
Clustering is an iterative method that begins with each data point interpreted as a single cluster and, in each iteration, the two most similar clusters are merged with respect to a linkage criterion until a single cluster remains.
There are many cluster linkage criteria used in the field, each with advantages and disadvantages. 
Perhaps the most common linkage may be the \emph{centroid linkage}, which defines cluster similarity as the distance between their centroids (the mean points). In this section, we will discuss the hierarchical clustering with the \emph{Mahalanobis linkage}, a more sophisticated method suitable for analyzing multidimensional single-cell cytometry datasets.

Mahalanobis linkage uses \emph{Mahalanobis distance}~\cite{mahalanobis1936generalized} to measure the similarity between clusters:
\begin{defn}[Mahalanobis distance]
    Suppose a probability distribution $C$ on $\R^d$ with mean $\bar{C} \in \R^d$ and a convariance matrix $\cov(C)$. If the matrix $\cov(C)$ is regular, we define the \emph{Mahalanobis distance} between $u \in \R^d$ and $C$ as
    \begin{equation}
    d_\text{Maha}(u,C) = \sqrt{(u-\bar{C})^T\cov(C)^{-1}(u-\bar{C})}.
    \end{equation}\label{eq01:maha}
\end{defn}

If we generalize a centroid of a cluster to a probability distribution, Definition~\ref{eq01:maha} can be used to define the Mahalanobis distance between a point and a cluster. To extend the definition to a distance between two clusters, we use the following equation~\cite{fivser2012detection}:
\begin{equation}
    \delta_\text{Maha}(P,Q) = \frac{d_\text{Maha}(\bar{P},Q) + d_\text{Maha}(\bar{Q},P)}{2}
\end{equation}\label{eq01:maha_linkage}

To illustrate the measure of the Mahalanobis distance, let us suppose we have two elliptic clusters. In the means of the proximity, the measure favors such clusters that their ellipses are alongside rather than in a prolongation of one another~\cite{dagnelie1991using} (see Figure~\ref{fig:ellipses}). 
Only when the objects of a cluster form a spherical shape, this dissimilairity measure is proportional to the Euclidean distance with a corresponding linkage.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/ellipses.pdf}
    \caption{Two pairs of clusters with different similarities according to Mahalanobis distance \cite{vsmelko2020gpu}}
    \label{fig:ellipses}
\end{figure}

\subsection{Background}

The prior work, which is our optimization based upon, is that of Fišer et al.~\cite{fivser2012detection}. They proposed the Mahalanobis hierarchical clustering as a valuable tool for analysis of flow cytometry datasets. Flow cytometric analysis is used (not exclusively) in minimal residual desease monitoring of hematologic malignancies. As a fast and sensitive method, it has been incorporated into several clinical trials of childhood acute lymphoblastic leukemia. 

However, the size of obtained flow cytometry datasets is constantly growing, which creates new challenges for the analysis. Modern flow cytometers are able to process samples with millions of cells and measure tens of parameters (point dimensions) simultaneously. More traditional methods, such as manual gating, are not suitable for such high-dimensional data and are heavily observer-dependent. Automated hierarchical clustering methods offer a viable solution but often fail to reflect the elliptical shapes of flow cytometric populations. 

For this purpose, the authors proposed the Mahalanobis linkage. This allows for a very natural formation of clusters in biological data. However, the benefits come at a cost of a high time and space complexity. The author claim the upper usable bound of dataset size to be $10^4$ using the original \texttt{C} application. The quadratic space complexity makes for the most significant limiting factor, making it unfeasible to analyze bigger datasets on current desktop computers.

The authors tried to alleviate this limitation by implementing the approximation of \emph{apriori clusters}, where the data was pre-clustered using a naive euclidean-based linkage reducing the overall size of the dataset. This increased the usability of the original code to datasets of around $10^6$ data points within an interactive environment~\cite{kratochvil2020shinysom}. Still, the proposed optimization could not keep up with the contemporary data aquisition methods, generating datasets of millions of multidimensional data points.



% We analyzed datasets with n = 10^4 events as the distance of data points requires O(n2) space in memory, and therefore, it becomes impractical (or even impossible) to analyze bigger datasets on current desktop computers.

% IN childhood acute lymphoblastic leukemia (ALL), response to therapy as measured by minimal residual disease (MRD) monitoring is an important biomarker for predicting relapse and stratifying treatment (1–5).

% , flow cytometry generates increasingly large and information-rich
% datasets, which provide new challenges for analysis. Modern multilaser flow cytometers are able to simultaneously measure up to 12 or more parameters and acquire
% such information from millions of single cells (8,9)

% Traditional gating of populations
% on two-parameter plots is tedious and does not reflect the multidimensionality of the data. Moreover, both the setting of the gates and interpretation of the results are observer-dependent and require intensive training and high levels of expertise. 

% Although hierarchical clustering methods
% can place each cell in its hierarchical context within the analyzed dataset, their downside is that they often fail to reflect
% the elliptical shapes of flow cytometric population

% However, the rapidly rising complexity of multiparameter
% flow cytometry datasets creates new challenges. Currently, the
% analytical standard in the field involves gating, in which one
% or more gates are defined in each histogram or dual parameter
% plot and a sequence or combination of gates defines the population of interest. This process is tedious or even unfeasible, already requires highly experienced cytometrists and is observer
% dependent. Most importantly, the sequential gating is limited
% in its ability to reflect the multidimensionality of the data. A
% solution to both the multidimensionality of flow data and the
% observer dependency lies in the usage of unsupervised learning
% methods.

% C version of the algorithm has been implemented within R package mhca and enhanced with the possibility of assuming
% apriori clusters for approximation, to reduce the unfavorable O(n3) time complexity for large datasets. That allowed the authors to process datasets of around
% 106 data points within an interactive environments



% problem - quadratic space complexity - even bigger problem with Mahalanobis distance and gpus 


\subsection{Algorithm complexity and the implementation}

From the perspective of GPU programming, Mahalanobis Hierarchical Clustering (HC) is a challenging problem. Algorithms with a high memory requirements are generally unfavorable for GPUs due to their limited DRAM. There has been a lot of work on overcoming this limitation, both from NVIDIA and the scientific community~\cite{zheng2016towards,kim2020batch,landaverde2014investigation}. CUDA has introduced \emph{unified memory}~\cite{site:cuda}, which allowed GPUs to work on data that exceeds GPU memory capacity by seamlesly copying data from CPU to GPU on page fault. This optimization can help only to a certain extent, as it can only scale to the size of CPU RAM and the data are usually sent through high-latency small-throughput interconnect. Therefore, in our work we experimented with other well-known HC variants which trade higher asymptotic time complexity for a linear memory complexity --- \emph{HC with the nearest neighbor array}~\cite{day1984efficient}. Furthermore, the execution on the real-world flow cytometry data showed that the average cost of the nearest neighbor array update is much lower than the worst case.


The other obstacle of Mahalanobis HC is the greatly imbalanced workload. In the first iterations, the runtime is dominated by the complex Mahalanobis linkage computation over many pairs of $n$ clusters. But as the number of clusters decreases, the hot-spot becomes the computation of the covariance matrix of a merged cluster, which has a time complexity of $\mathcal{O}(d^2 \cdot c)$, where $d$ is the number of dimensions of the data and $c$ is the size of the cluster. It could happen on many inputs, that for the most time during the computation, the GPU is not fully utilized either due to small amount of parallelism in covariance matrix computation or in the similarity computation. For that case, we designed a workload which uses CUDA \emph{streams}~\cite{site:cuda} that enables overlap the execution of mutliple GPU kernels and memory transfers. This allowed us to keep the utilization of more GPU cores during the whole runtime of the algorithm.

As a result, the optimized Mahalanobis HC achieves a speedup of over $1400\times$ compared to the original serial CPU implementation. We benchmarked the application on the real-world single-cytometry datasets. The biggest dataset which we obtained, the Samusik-All~\cite{flowrepo} ($841$K points of $39$D), was able to finish in the order of minutes compared to the order of days. The application has been distributed as a R package \texttt{gmhc} to fit a workflows of carrried out by a bioinformatics comunity. As of our knowledge, the package enabled the scientists to analyze big datasets as a whole, without the apriori clustering approximation, increasing the accuracy of the analysed data and decreasing the turnaround time of the analysis.  

% Hierarchical Clustering (HC) is a well-known problem, and there are many algorithms that effectively solve it. Roughly, they can be divided into categories according to the data structures they employ [TODO]:
% \begin{itemize}
%     \item HC with a dissimilarity matrix --- the most straightforward approach; the dissimilarity matrix stores the distances between all pairs of clusters. This approach has a cubic time complexity and quadratic memory complexity.
%     \item HC with a nearest neighbor array --- each cluster stores the index of its nearest neighbor. This approach trades a linear memory complexity for an asymptotic time complexity of $\mathcal{O}(d \cdot n^3)$.
%     \item HC with an array of priority queues --- a more sophisticated solution, which reduces the time complexity to $\mathcal{O}(n^2 \log n)$.
% \end{itemize}

% C application \texttt{mhclust}, the original implementation of the Mahalanobis HC, uses the dissimilarity matrix. This approach is simple and allows for a straightforward implementation. However, the empirical data reveal the most significant limiting factor in the quadratic memory complexity. The Mahalanobis distance adds a multiplicative factor in $\mathcal{O}(d^2)$ to the memory complexity, so usually, the machine runs out of memory before the execution time reaches unreasonable limits.

% As a solution to this limitation, we can assume \emph{in-place} variant of the HC with dissimilarity matrix, which discards the whole matrix and chooses the closest cluster pair by computing the distances ad-hoc. This increases the asymptotic time complexity to $\mathcal{O}(d \cdot n^3)$ assuming the Euclidean distance with centroid linkage.

% Our work used the nearest neighbor array approach to achieve fewer distance computations per iteration on average while keeping the memory complexity linear. Notably, the algorithm complexity per iteration is quadratic only in the worst case, when each cluster's neighbor needs to be reevaluated. This case would correspond to constantly merging such cluster pairs that all the other clusters are neighboring. In practice, this is an atypical case, and the empirical data show that the number of neighbors to reevaluate is generally in the range of 10 to 100 for tested datasets with at most 1M of points.

% Apart from the complex distance computation, the Mahalanobis HC suffers from the unbalanced nature of the algorithm. In the first iterations, the runtime is dominated by the distance computation in the data structure update. However, when the number of clusters decreases, the bottleneck becomes the covariance matrix computation of a merged cluster (with a time complexity of $\mathcal{O}(d^2 \cdot n)$). We used \emph{streams}, a CUDA abstraction for task parallelism on GPUs, to overlap the covariance matrix computation with the data structure update to keep GPU utilization high.

% \subsection{Future work}

% The Mahalanobis HC is a valuable method for analyzing cytometry data. However, since we published the optimized algorithm, there have been new additions to the GPU CUDA framework, which could be used to improve the performance of the algorithm further. Specifically, the whole Mahalanobis distance could be computed as a single \emph{Tensor} operation, drastically improving the application throughput. Another promising optimization would be to use \emph{CUDA Graph} to reduce the kernel launch overhead.


\section{Neighborhood-based dimensionality reduction}

% Another approach to the analysis of multidimensional point-like cloud datasets is to reduce the dimensionality of the data to a lower (human-readable) dimension while preserving the local structure of the data. This approach is used in many areas of life sciences, such as microscopy imaging, population biology, and others. The most popular methods for dimensionality reduction are t-SNE and UMAP. Both these methods are nonlinear neighborhood embeddings and use a stochastic approach to find the optimal mapping. However, their applicability reaches a limit when increasing the scale or requiring real-time processing.

% The EmbedSOM algorithm tries to tackle this limitation. It is based on self-organizing maps and uses a landmarks-based approach to reduce the computational complexity. Instead of computing all pairwise distances and stochastically optimizing the low-dimensional mapping, EmbedSOM uses a small set of landmarks to approximate the mapping. This allows for over a magnitude better performance while retaining the reasonable accuracy of the results. In our works, we followed up on the EmbedSOM algorithm and implemented it on GPUs to achieve real-time processing of large datasets, allowing for a semi-supervised dataset analysis.


Complementary to hierarchical clustering, a different approach to displaying cytometry datasets is \emph{dimensionality reduction} (also called \emph{embedding}), in which multi-dimensional cells are projected into a 2-dimensional plane, a picture, which shows cells arranged in groups with common properties. This methodology allows for a fast and reliable way to analyze cell populations, their relative size, and the presence of various features. The currently used dimensionality reduction tools are typically based on the principle of optimizing a low-dimensional embedding while preserving high-dimensional properties of interest. The most popular tools following this methodology, such as t-SNE~\cite{maaten2008visualizing}, UMAP~\cite{becht2019dimensionality} or TriMAP~\cite{amid2019trimap}, can suffer poor performance on large data due to the need to examine a nontrivial subset of $\binom{n}{2}$ relations (such as the pairwise distances) between $n$ data points~\cite{kratochvil2019generalized}.

A lot of effort has been dedicated to optimizing the performance of these algorithms. Solely for t-SNE, we can find multiple works related to this topic~\cite{pezzotti2016hierarchical,pezzotti2016approximated,linderman2017efficient,belkina2018automated}. Regardless of these developments, the processing time of these algorithms scales super-linearly with the number of data points, which inevitably leads to the need of downsampling the data. \emph{EmbedSOM} agorithm, introduced by Kratochvíl et al.~\cite{kratochvil2019generalized}, is designed to overcome this limitation. The costly parts of the previous methods can be omitted by creating a smaller model of the data obtained (not exclusively) by \emph{self-organizing maps}. EmbedSOM uses the information of such an approximated manifold to compute the final embedding, retaining a competitive quality of the visualization. 

Algorithm~\ref{alg01:esom} shows the overview of EmbedSOM. It assumes a set of $n$ high-dimensional points $X \in \R^{n \times d}$ and the smaller data model: a set of $g << n$ high-dimensional landmarks $L \in \R^{g \times d}$, and a set of $g$ low-dimensional landmarks $l \in \R^{g \times 2}$. For each input point, $k < g$ nearest landmarks from $L$ are found and assigned scores according to their distance. Finally, we compute the embedding such that the difference between distances from $l$ landmarks and the embedded point and $L$ landmarks and the original point is minimized. The minimization problem is reducible to a linear system of equations with two variables.

\begin{algorithm}[t]
    \caption{EmbedSOM}
    \label{alg01:esom}
    \begin{algorithmic}[1]
        \Procedure{EmbedSOM}{$X\in\R^{n\times d}$, $L\in\R^{g\times d}$, $l\in\R^{g\times 2}$, $k\in\N$}
        \For {$i \in \{1\dots n\}$} \Comment{For each high-dimensional point}
            \State Find $k$ nearest landmarks from $L$
            \State Score the $k$ nearest landmarks according to the distance to $X_i$ with $s_1, \dots, s_k$
            \For {$(u,v) \in \{1,\dots,k\}^2$ } \Comment{For each pair of nearest landmarks}
                \State Compute $D_{uv}(X_i)$ by projecting $X_i$ orthogonally onto a line between $L_u$ and $L_v$; We define $d_{uv}(x)$ similarly for $l_u$ and $l_v$
            \EndFor
            \State Find $x_i$, such that $\sum_{u, v} s_u \cdot s_v \cdot (D_{uv}(X_i) - d_{uv}(x_i))$ is minimized
        \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

With such a description of the algorithm, we can deduce the time complexity for a single data point processing: The first line of the for loop is the well-known $k$-NN problem, with the optimal time complexity of $\mathcal{O}(d \cdot g \cdot \log k)$. The remainder, which we may call the embedding step, has a time complexity of $\mathcal{O}(d \cdot k^2)$. Since it holds that $k < g << n$, EmbedSOM achieves sufficient scaling; the embedding of $24$ million cells with $36$ markers can finish under an hour on common hardware, compared to around $2$ days using UMAP~\cite{kratochvil2019som}.

\subsection{Interactivne opportunities of GPU implementation}

Although the EmbedSOM CPU implementation already provided good performance, it still had an untapped parallelization potential and a potential for enablement as an interactive method in cytometry data analysis. Therefore, we followed up on it in our work. 

The main challenge of EmbedSOM is low \emph{arithmetic intensity} --- the number of math operations per transferred byte. A huge math bandwidth of a GPU can only be achieved if the device can keep up transferring data from memory to the cores. Low arithmetic intensity indicates that data transfers will not keep up with the math bandwidth.

Note that although the arithmetic intensity is primarily determined by the algorithm, it can be influenced by its implementation in a major way. For GPUs, there are many ways to programmatically increase the arithmetic intensity of an algorithm, such as kernel fusion~\cite{wahib2014scalable}, leveraging the memory hierarchy~\cite{lee2012cuda} or by reordering data accesses~\cite{ghysels2012improving} and optimizing data transactions~\cite{lu2020optimizing}. Generally, these approaches can be distilled into two groups:
\begin{itemize}
    \item \emph{Data reuse} --- A single datum is reused for as many operations as possible. This is especially useful for algorithms that load the same data multiple times for different computations. 
    \item \emph{Latency hiding} --- Instead of waiting for data to arrive, GPU context-switches to different threads to perform other operations. GPUs are designed to schedule many more threads than the number of cores with very little overhead. With such work overlapping, the high memory latencies can be completely hidden.
\end{itemize}

We experimented with both approaches in our EmbedSOM implementation. Although caches can partially handle data reuse, GPUs typically hardly benefit from them due to their low cache-to-core ratio~\cite{site:cuda}. Therefore, as one variant of $k$-NN part, we used \emph{shared memory}, a programmable cache, to store the data and landmarks. These were then used to compute all the possible pairwise distances before loading the next batch to maximize the arithmetic intensity. The second variant of the $k$-NN part used a highly parallel \emph{bitonic sort} to find the top-$k$ landmarks. This approach has the hidden benefit of low per-thread resource requirements, resulting in higher maximum GPU occupancy and, consequently, better latency hiding.

The embedding part of EmbedSOM does not offer such a data reuse opportunity, as each point has a different set of nearest landmarks. The only reuse can happen on the landmarks themselves. And since their count can be limited in some parameter configurations, we experimented with techniques that increase the memory bandwidth, such as \emph{vector load instructions}. 

After the thorough benchmarking process, we selected the most performing variants of both steps and combined them into a complete implementation of the EmbedSOM algorithm. We increased the performance by $200-1000\times$ over the serial CPU version and $3-10\times$ over a naive GPU implementation. Furthermore, the achieved speedup enabled the interactive data visualization and was integrated into the graphical application \emph{BlosSOM}. This application can project datasets of up to a million points with a maintained frame rate above $30$ frames per second on consumer hardware. This contribution pushes the boundary of EmbedSOM into a semi-supervised dimensionality reduction domain, allowing users to effectively and intuitively visualize data with real-time feedback.


% EmbedSOM projection can be viewed as an embedding enrichment method: From a set of landmarks in the 
% high-dimensional space and a set of corresponding landmarks in the low-dimensional space, it produces a smooth 
% function that maps all points from the higher-dimensional space to the low-dimensional space and preserves 
% the relative neighborhoods of the landmarks. EmbedSOM was originally designed to work with simple SOMoriginating landmarks, as shown in Figure 1.

% Ecient unbiased data analysis is a major challenge for laboratories handling
% large ow and mass cytometry datasets. We present EmbedSOM, a non-linear
% embedding algorithm based on FlowSOM that improves the analysis by providing high-performance embedding method for the cytometry data. e algorithm
% is designed for linear scaling with number of data points, and speed suitable for
% interactive analysis of millions of cells without downsampling. At the same time,
% the visualization quality of single cell distribution within cellular populations and
% their transition states is competitive with the current state-of-the-art algorithms.
% We demonstrate EmbedSOM properties on two use-cases, showing benets of using the interactive algorithm speed in supervised hierarchical dissection of cell
% populations, and the scalability improvement by eciently processing very large
% datasets.

% e ever-increasing size and dimensionality of data generated by ow and
% mass cytometry experiments drive interest in simplifying data analysis. Employing the usual repetitive manual gating, exploration and back-gating techniques is tedious if the sample count is high, and becomes imprecise on complex
% 20 data. During the past decade, a multitude of automated analysis methods have
% been introduced, including various unsupervised clustering and phenotyping algorithms, and embedding methods. Comprehensive reviews of the algorithms
% are available [19, 27, 10, 11]

% The preferred method to display cytometry datasets is embedding, in which
% 25 cells are arranged into a 2-dimensional picture showing populations of agglomerated cells with similar properties. is provides a straightforward way to inspect
% the relative population sizes, their contents, and the presence of various features
% including major subpopulations, intermediate cell states and trajectories of their
% development. e performance of available embedding algorithms is constantly
% 30 being improved. For example, the widely used tSNE [24] has formed the basis
% for faster ASNE [17] and HSNE [16], and was further improved in OptSNE and
% FItSNE [3, 14] and accelerated using GPU by Chan et al. [5]. Other methods include SWNE, scvis, largevis [29, 6, 8], and the relatively new UMAP [15] that
% specically aims to provide beer, faster embedding than tSNE. Despite these
% 35 developments, two key objectives have not been met:
% • e embedding algorithm should be able to process the data of volumes
% common in cytometry quickly, ideally within seconds, to allow interactive
% data inspection;
% • algorithm processing time should scale linearly with the number of cells,
% 40 to be able to keep up with the increasing sizes of datasets without downsampling.
% We introduce EmbedSOM, a new embedding algorithm that is designed to
% satisfy these two requirements. e algorithm uses a self-organizing map (SOM)
% that describes the multidimensional cell space. SOMs have been successfully
% 45 used for classifying this space into clusters — for example, FlowSOM [25] uses
% SOM vertices as cluster centers to classify the cells into clusters that form a basis
% for further analysis. e SOM additionally approximates a section of a smooth
% 2-manifold embedded in the multidimensional space in a manner such that the
% cells are uniformly distributed in its neighborhood. EmbedSOM uses this addi50 tional information to compute the embedding by ing a projection of each cell
% 2
% onto this manifold, and transforming the projection coordinates to 2-dimensional
% SOM-relative coordinates.
% e performance-oriented design of EmbedSOM diers substantially from
% other commonly used embedding methods. Most importantly, the usual time55 consuming iterative optimization of single cell positions in the embedding is replaced by relatively fast manifold approximation by SOM training. Additionally,
% the separation of the SOM-training from other stages of the algorithm introduces exibility that allows precise manipulation of separate embedding properties, such as reliable alignment of cell populations in the embedding even with
% 60 substantially dierent samples, and easy optimization of the embedding layout

% 2.1. EmbedSOM provides superior embedding speed
% e main distinctive feature of EmbedSOM is its computational eciency. A
% dataset of common size (approximately 300k cells and 20 markers) can be mapped
% by the SOM and embedded in less than a minute; the GPU-accelerated versions of
% the algorithms deliver the same result in seconds. Generally, embedding datasets
% 80 with millions of cells and several dozen markers is possible in minutes using
% common oce hardware. Moreover, since the major SOM-training part of the
% required computation is shared with FlowSOM, EmbedSOM visualization adds
% only minor computational complexity to workows that already use FlowSOM.
% antitative measurements of the speed advantage on the benchmark com85 putations are displayed in Figure 1a. e results conrm the expected performance scaling gap between UMAP and EmbedSOM, caused by dierent algo3
% rithm design. Generally, UMAP and tSNE are very ecient for high-dimensional
% data with a low data point count, because they remove the dimensionality overhead early in the process but require more than linear amount of computation
% 90 to optimize the nal data point positions. e performance of both EmbedSOM
% stages is linear in the number of data points, but the dimensionality overhead is
% present throughout almost the entire computation as another linear factor.
% EmbedSOM benets from this trade-o when applied to high-volume ow
% and mass cytometry data that are physically limited to dozens of dimensions.
% 95 Conversely, UMAP remains faster on low-volume datasets with more than hundreds of dimensions. For example, the raw single-cell RNA sequencing data must
% be pre-processed (e.g. reduced to principal components) in order to deliver comparable results with SOM-based algorithms [7].
% Overall, the performance scaling dierence is best illustrated by the compu100 tation time required for running workows used later in the article: Embeddingassisted dissection of 1 million cells (Section 2.3) takes less than 5 minutes of
% computation with EmbedSOM; but more than 1 hour with UMAP. Embedding of
% the 24 million cells from Pregnancy dataset (Section 2.4) can be nished faster
% than in 1 hour with EmbedSOM, but requires around 2 days with UMAP.

% The SOM is used for multiple purposes: an embedded single-cell
% view of the samples is computed from the SOM using the fast
% EmbedSOM algorithm (Kratochvı´l et al., 2019). Clustering of the
% cells is conducted in an interactive dendrogram (Sieger et al.,
% 2017) built from hierarchical metaclustering of the SOM content.
% Notably, ShinySOM supports the agglomerative clustering with
% Mahalanobis-average linkage (Fiser et al., 2012) that has been
% shown to be capable of defining cell populations with the same accuracy as expert cytometrists in a real life setting. After selecting the
% cell populations, the user may display and analyze the differences
% between samples and sample groups, and visualize the results of
% statistical testing of the differences in cell abundance. Finally, the
% annotated populations may be partitioned into smaller datasets, in
% order to be explored in more detail.


\section{Cross-correlation optimized for small inputs}

Leaving the realm of cytometry data analysis, we will focus on an extremely data-bound problem, the cross-correlation. This algorithm is a cornerstone of many scientific fields, such as image processing, seismology, particle physics, and with the advent of convolutional neural networks, also in machine learning. As one iterpretation, it computes a similarity of two series of data by sliding one over the other and computing the dot product of the overlapping parts. The output is typically post-processed to search for a specific feature, find the displacement, 

% fyzici
% definicia

% aritmeticka intenzita
% register sharing
% workload imbalance


We have already alluded that arithmetic intensity is a crucial factor in the performance of GPU algorithms. The algorithm described in this section stresses the importance of data caching when working with highly parallel machines, especially when dealing with data-bound problems.

Cross-correlation plays a major role in signal processing and can be found in many fields of science, such as image processing, seismology, particle physics, etc. It is used to quantify the dissimilarity between two signals by computing a measure of similarity between all their shifts. The cross-correlation of two discrete functions $f$ and $g$ is defined as
\begin{equation}
    (f \star g)(\tau) = \sum_{i=-\infty}^{\infty} f(i) g(i+\tau).
\end{equation} 

Figure~\ref{fig:cross-correlation} depicts the visual representation of the equation applied to a 2-dimensional discrete case with two matrices. Although the formula is quite simple, the complexity of the 2D variant reaches $\mathcal{O}(w^2 \cdot h^2)$, where $w$ and $h$ are widths and heights of input matrices, respectively. Luckily, cross-correlation can be reduced into a problem solvable by a Fourier algorithm [TODO] with a more pleasing time complexity of $\mathcal{O}(w\cdot h \cdot \log(w\cdot h))$. Yet, the hidden multiplicative factor, which materializes as an overhead in the implementations of the Fast Fourier Transform (FFT), favors the definition-based approach for small problem sizes.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{img/cc.png}
    \caption{Visual representation of the cross-correlation. Yellow and purple matrices correspond to the input matrices, and the blue matrix depicts the cross-correlation output. The coordinates on the blue matrix correspond to the shift of the yellow matrix, which is depicted on the right. Only the overlapping parts (in pink) contribute to the computation for each output matrix element.}
    \label{fig:cross-correlation}
\end{figure}

\subsection{Implementation}

The definition-based approach is a straightforward implementation of the cross-correlation formula. It is a simple algorithm that computes the dot product for all $(2w - 1) \cdot (2h -1)$ overlaps of input matrices. The algorithm is highly parallelizable but suffers from a relatively significant workload imbalance (since each overlap comprises a different number of matrix elements) and a low baseline arithmetic intensity if we do not consider any data-sharing techniques. 

\paragraph{Irregular workload} In our work, we addressed this issue by experimenting with different workers. Having multiple levels of parallelism on contemporary GPUs allows us to experiment with different `work groups' to assign them to a single overlap computation. Generally, we can choose from a \emph{warp}, a \emph{block}, or a \emph{grid} of threads on CUDA platform (in the hierarchically ascending order)\footnote{The most recent versions of CUDA introduces one more group of threads hierarchically located between a block and a grid called a \emph{block cluster}.}. They all have different ways to synchronize and share data, with a typically increasing penalty as we ascend the hierarchy. Notably, threads in a warp support data-sharing constructs within the fastest memory on a GPU --- the register file. 

In our work, we analyzed the memory traces produced by fetching the data during the computation of the neighboring overlaps. A usable memory pattern emerges if the overlapping data is read in a typical row-major pattern. A thread $t$ in iteration $i$ requires the same data as a thread $t-1$ at iteration $i-1$. Such a pattern favors the warp-level parallelism with high-throughput shuffle intrinsics, which allow for shuffling data within thread registers. In our work, we utilized the \emph{register buffer} to read ahead the data for immediate sharing.

Such work distribution enabled a moderate baseline for data sharing, increasing the arithmetic intensity of the algorithm. We also experimented with other work groups, such as assigning multiple threads or the whole block to a single overlap computation. This helped when the overall achievable parallelism was low when processing extremely small inputs.

\paragraph{Data sharing} To further increase the arithmetic intensity, we experimented with other data-sharing techniques that the problem allowed, such as sharing the same overlapping elements between neighboring overlaps. In a \emph{one-to-many} use case, where cross-correlation is computed between one left matrix and multiple right matrices, data loaded from the left matrix can be shared among multiple right matrix overlaps. Respectively, in the \emph{n-to-m} use case, the same can be done with the right matrix. 

Notably, all the mentioned sharing techniques are orthogonal to each other and can be freely combined. Naturally, the growing register requirements with each combined technique are bounded by a limit in the size of the register file (typically around 64KB) and the number of registers, which can be assigned to a single thread by a compiler. If the limit is reached, occupancy drops or register spilling occurs. Apart from that, one also has to consider the decreased parallelism that inherently comes with data sharing.

Therefore, to find the perfect balance between data sharing and the level of parallelism, we conducted a thorough benchmarking process and created the optimization matrix for the cross-correlation algorithm. It generally showed, that the high-overhead FFT implementation provides plenty of space for the definition-based approach to shine, even on a moderately sized n-to-m problem instances.


\section{Stochastic simulation of Boolean networks}

In the last section of this chapter, we will discuss the challenges of porting a stochastic simulation of Boolean networks to GPUs. 

another domain of scientific algorithms

the mission is the same -> enable more research by improving the performance

describe the simulation

talk about the challenges  
- iteration step is not suitable for GPUs 










% thesis is organized as follows

% tam pridat paragraf ku kazdemu clanku

% a az na koniec 1

% viac zrovnanie

% preco su tieto alg tazke

% pitfally co sa stanu ked implementujes na gpu

% v inter tiez dokladne povedat preco to mame rozdelene na scientific aglos + noarr



% gPu science
% - ukazat preco je to zaujimave
% - ze to je potrebne pre biologov
% - ze to zlepsuje situaciu pre vyskum
% - ze je to zaujimavy problem - takze asi spomenut ake boli doterajsie limitacie
% - ukazat ze to je fakt tazky - popisat, co si to vyzadovalo, ake to malo problemy, ma to vela moznosti kde to paralelizovat, vsetky popisat 
% - ale na vysokej urovni - zhrnut a vysvetlit ze to je tazke
% - na zaver poplacat na zadech - zrychlili sme to 1000x

% - future work skor nie 
% - 

% ku intur hodit referencie idealne surveye
