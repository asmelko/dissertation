\section{Experimental Methodology}\label{appendix:methodology}

The main objective of the benchmarking was to measure the speedups achieved by different layout combinations to support the claims mentioned in the work\footnote{More details and the data are in the replication package \url{https://github.com/asmelko/ica3pp22-artifact}.}. A more complex performance evaluation is beyond the scope of this paper and is planned in future work.

% -----------------------------------------------------------------------------
\subsection{GPU benchmarking setup}
% -----------------------------------------------------------------------------

In the results, we present mainly the kernel execution times measured by the high-precision system clock, which is available on all platforms. The relative standard deviations in $20$ collected measurements of each result were less than $5$\% of the mean value in all cases, so we report only the mean values.

Due to the page limit, the presented results were limited to matrices of sizes $(1008\times 1008)$ and $(10,080\times 10,080)$. However, more extensive testing on other problem instances, including a broader range of matrix sizes and non-square matrices, exhibited similar results.

The results were collected on the following platforms:
\begin{itemize}
    \item NVIDIA Tesla V100~SXM2 (Volta, CC 7.0, $1.3$GHz), Rocky Linux 8
    \item NVIDIA GeForce RTX 2060 (Turing, CC 7.6, $1.7$GHz), Windows 10
    \item NVIDIA GeForce RTX 3070 laptop (Ampere, CC 8.6, $1.6$GHz), Windows 11
\end{itemize}

All platforms used CUDA toolkit 11.6 with an up-to-date driver. These devices represent three of the most recent Nvidia architectures and three typical hardware platforms (server, desktop PC, and laptop). Hence, we claim that the measurements sufficiently represent contemporary CUDA-enabled GPUs.


% -----------------------------------------------------------------------------
\subsection{CPU benchmarking setup}
% -----------------------------------------------------------------------------

We ran the kernel in $100$ iterations for the stencil benchmark, plotted the local regression outlining the mean value, and distinguished the outliers.
The measurements were conducted using the following CPUs:
\begin{itemize}
    \item AMD Ryzen 5 5600X (hi-end desktop CPU, $3.70$GHz), Windows 10
    \item Intel Core i7-10870H (laptop CPU, $2.20$GHz), Windows 11
    \item Intel Xeon Gold 5218 (server CPU, $2.3$GHz), Rocky Linux 8.
\end{itemize}

Due to the fact that some compilers may optimize \texttt{constexpr} expressions better than others, we compiled the benchmark using \texttt{clang++} v$12$ and \texttt{g++} v$11$ compilers with \texttt{-03} flag.
We also compiled the stencil benchmark using the MSVC C++ compiler, but the results showed that it could not sufficiently optimize \Noarr{} code in the current version; hence, MSVC results are not included.

All benchmarking datasets were synthetic, with data sampled randomly from the same uniform distribution. We consider synthetic validation sufficient since the performance of the benchmarked algorithms is not data-dependent.
