\chapwithtoc{Introduction}

General-purpose GPU programming (GPGPU programming) has established a strong position in the domain of high-performance computing (HPC).
GPU programming enables the parallel execution of many threads on a single device, exploiting the massive computational power of modern GPUs.
These devices are no longer used exclusively for graphics rendering, and since the advent of the CUDA framework (and other frameworks alike), they have become a viable alternative to CPUs for high-performance computing~\cite{khairy2019survey}.
Nowadays, the top data centers in the world are equipped with thousands of GPUs~\cite{top500}, being used for a wide range of tasks, including machine learning, data analytics, and scientific computing~\cite{mittal2019survey,bress2014gpu,kalaiselvi2017survey}.

Naturally, GPUs are not a silver bullet as they are not suitable for any task.
GPU programming poses many challenges, including complex hardware architectures, heterogeneous programming models, memory management, performance tuning, and portability.
The ever-growing list of new CUDA features, such as dynamic parallelism~\cite{wang2014characterization}, asynchronous data copy~\cite{pearson2019evaluating}, independent memory pools~\cite{qian2023empirical}, kernel launch buffering~\cite{lin2021efficient}, distributed shared memory~\cite{choquette2023nvidia}, tensor cores~\cite{dakkak2019accelerating}, and many more, is a double-edged sword;
it aids programmers in achieving peak GPU performance in their applications but at the cost of increased programming complexity and an expert architecture knowledge requirement.
Generally, GPGPU programming is an iterative process of developing a solution, profiling it, and optimizing the bottlenecks.
Apart from that, there is generally \emph{no single optimal solution} to the problem. The programmers must develop multiple variants that must be empirically tested to find the most suitable optimization for a specific set of input parameters~\cite{hijma2023optimization}.

This thesis contributes to addressing the challenges of GPGPU programming in two interconnected topics:
Firstly, the thesis couples a set of data-intensive scientific algorithms and their GPU optimizations; each contains a detailed discussion of their concurrency opportunities, memory operation analyses, and the choice of the most suitable optimization variant. Each work employs a slightly different tool in the GPU programming toolbox.
In the second part of the thesis, we introduce the Noarr library, which builds upon the findings and expertise gained from the results of the previous part. We identified that the primary factor of many optimizations is the order of accessing data in the memory, generally referred to as \emph{memory optimizations}.
Noarr adds memory layouting primitives and provides a way to compose and traverse them in a customizable manner. The main goal of the library is to aid in writing more expressive, maintainable, and modular HPC code.

\paragraph{Structure of the thesis}

The thesis is separated into two chapters. \cref{chap:gpu} focuses on the GPU optimizations of four scientific algorithms. The chapter starts with \cref{sec:gpu_intro}, which provides an introduction to GPU programming and highlights the most significant performance-related pitfalls. \cref{sec:maha} describes the first scientific algorithm, the Hierarchical Clustering algorithm with Mahalanobis-based distance, and details the ways of overcoming problems of imbalanced workloads. In \cref{sec:embedsom}, EmbedSOM, a dimensionality reduction algorithm, is discussed from the perspective of caching and utilization of multiple memory hierarchies. Further, we detail the importance of keeping high arithmetic intensity when implementing a cross-correlation algorithm in \cref{sec:cross-correlation}. We conclude the chapter with \cref{sec:maboss}, which describes MaBoSS, a Monte Carlo simulator of biological systems, and the runtime compilation capabilities of GPUs. \cref{chap:noarr} introduces the Noarr library. The chapter is logically divided into two subparts: the techniques of expressing memory layouts as first-class citizens (\cref{sec:layouts}) and the ways of traversing them in a customizable manner (\cref{sec:traversals}). \cref{chap:contributions} contains the peer-reviewed publications which support the contributions of this work.
